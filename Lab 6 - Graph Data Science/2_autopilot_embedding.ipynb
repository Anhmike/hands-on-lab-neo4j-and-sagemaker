{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rQ6G65n6OxV"
   },
   "source": [
    "# Autopilot Embedding\n",
    "In this notebook, we'll use SageMaker to train a model using our graph embedding as an additional feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Connection\n",
    "Let's setup our SageMaker connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/form13'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Amazon S3\n",
    "Now we're going to upload the training and testing data to our default SageMaker bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_s3_path = session.upload_data(path='train.csv', key_prefix=prefix + '/train')\n",
    "print('Training data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_data_s3_path = session.upload_data(path='test.csv', key_prefix=prefix + '/test')\n",
    "print('Testing data uploaded to: ' + test_data_s3_path)\n",
    "\n",
    "validation_data_s3_path = session.upload_data(path='validate.csv', key_prefix=prefix + '/validate')\n",
    "print('Validation data uploaded to: ' + validation_data_s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the SageMaker Autopilot Job\n",
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml_job_config = {'CompletionCriteria': {'MaxCandidates': 3}}\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': 's3://{}/{}/train'.format(bucket, prefix),\n",
    "            }\n",
    "        },\n",
    "        'TargetAttributeName': 'target',\n",
    "    }\n",
    "]\n",
    "\n",
    "output_data_config = {'S3OutputPath': 's3://{}/{}/output'.format(bucket, prefix)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the SageMaker Autopilot Job\n",
    "You can now launch the Autopilot job by calling the create_auto_ml_job method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "auto_ml_job_name = 'automl-embedding-' + timestamp_suffix\n",
    "print('AutoMLJobName: ' + auto_ml_job_name)\n",
    "\n",
    "sm.create_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    AutoMLJobConfig=auto_ml_job_config,\n",
    "    RoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking SageMaker Autopilot job progress\n",
    "SageMaker Autopilot job consists of the following high-level steps : * Analyzing Data, where the dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets. * Feature Engineering, where Autopilot performs feature transformation on individual features of the dataset as well as at an aggregate level. * Model Tuning, where the top performing pipeline is selected along with the optimal hyperparameters for the training algorithm (the last stage of the pipeline).\n",
    "\n",
    "In our experience, this job takes anywhere from 20-80 minutes to run.  At this point, you might want to set the rest of this notebook to run and move on to the raw notebook, \"3_autopilot_raw.ipynb.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('JobStatus - Secondary Status')\n",
    "print('------------------------------')\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "print(describe_response['AutoMLJobStatus'] + ' - ' + describe_response['AutoMLJobSecondaryStatus'])\n",
    "job_run_status = describe_response['AutoMLJobStatus']\n",
    "\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "\n",
    "    print(\n",
    "        describe_response['AutoMLJobStatus'] + ' - ' + describe_response['AutoMLJobSecondaryStatus']\n",
    "    )\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Now use the describe_auto_ml_job API to look up the best candidate selected by the SageMaker Autopilot job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "\n",
    "print('CandidateName: ' + best_candidate_name)\n",
    "print('FinalAutoMLJobObjectiveMetricName: ' + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print('FinalAutoMLJobObjectiveMetricValue: ' + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))\n",
    "print()\n",
    "pprint.pprint(best_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference\n",
    "Now that we completed the SageMaker Autopilot job on the dataset, let's create a model from the best candidatewith Inference Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'automl-embedding-model-' + timestamp_suffix\n",
    "model = sm.create_model(Containers=best_candidate['InferenceContainers'], ModelName=model_name, ExecutionRoleArn=role)\n",
    "print('Model ARN corresponding to the best candidate is: {}'.format(model['ModelArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use batch inference through Amazon SageMaker batch transform. The same model can also be deployed to perform online inference using Amazon SageMaker hosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_job_name = 'automl-embedding-transform-' + timestamp_suffix\n",
    "\n",
    "transform_input = {\n",
    "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': test_data_s3_path}},\n",
    "    'ContentType': 'text/csv',\n",
    "    'CompressionType': 'None',\n",
    "    'SplitType': 'Line',\n",
    "}\n",
    "\n",
    "transform_output = {\n",
    "    'S3OutputPath': 's3://{}/{}/inference-results'.format(bucket, prefix),\n",
    "}\n",
    "\n",
    "transform_resources = {'InstanceType': 'ml.m5.4xlarge', 'InstanceCount': 1}\n",
    "\n",
    "sm.create_transform_job(\n",
    "    TransformJobName=transform_job_name,\n",
    "    ModelName=model_name,\n",
    "    TransformInput=transform_input,\n",
    "    TransformOutput=transform_output,\n",
    "    TransformResources=transform_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the transform job for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('JobStatus')\n",
    "print('----------')\n",
    "\n",
    "describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "job_run_status = describe_response['TransformJobStatus']\n",
    "print(job_run_status)\n",
    "\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "    job_run_status = describe_response['TransformJobStatus']\n",
    "    print(job_run_status)\n",
    "    sleep(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now letâ€™s get the URL of the transform job results.  You can open this in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = session.default_bucket()\n",
    "key = '{}/inference-results/test_data.csv.out'.format(prefix)\n",
    "url='s3://' + bucket + key\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View All Candidates\n",
    "You can view all the candidates (pipeline evaluations with different hyperparameter combinations) that were explored by SageMaker Autopilot and sort them by their final performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = sm.list_candidates_for_auto_ml_job(AutoMLJobName=auto_ml_job_name, SortBy='FinalObjectiveMetricValue')['Candidates']\n",
    "index = 0\n",
    "for candidate in candidates:\n",
    "    print(\n",
    "        str(index)\n",
    "        + '  '\n",
    "        + candidate['CandidateName']\n",
    "        + '  '\n",
    "        + str(candidate['FinalAutoMLJobObjectiveMetric']['Value'])\n",
    "    )\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Generation Notebook\n",
    "Sagemaker AutoPilot also auto-generates a Candidate Definitions notebook. This notebook can be used to interactively step through the various steps taken by the Sagemaker Autopilot to arrive at the best candidate. This notebook can also be used to override various runtime parameters like parallelism, hardware used, algorithms explored, feature extraction scripts and more.\n",
    "\n",
    "The notebook can be downloaded from the following Amazon S3 location:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['AutoMLJobArtifacts']['CandidateDefinitionNotebookLocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Notebook\n",
    "Sagemaker Autopilot also auto-generates a Data Exploration notebook, which can be downloaded from the following Amazon S3 location:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['AutoMLJobArtifacts']['DataExplorationNotebookLocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code deletes them. This operation deletes all the generated models and the auto-generated notebooks as well.\n",
    "\n",
    "The lines below are currently commented out.  Uncomment them if you'd like to run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 = boto3.resource('s3')\n",
    "#bucket = s3.Bucket(bucket)\n",
    "\n",
    "#job_outputs_prefix = '{}/output/{}'.format(prefix, auto_ml_job_name)\n",
    "#bucket.objects.filter(Prefix=job_outputs_prefix).delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "embedding.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
